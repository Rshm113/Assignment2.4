Ans 2 :- Explain the components of Hadoop framework
Apache Hadoop is an open-source software framework written in Java. It is primarily used for storage and processing of large sets of data,
better known as big data. It comprises of several components that allow the storage and processing of large data volumes in a clustered
environment. However, the two main components are Hadoop Distributed File System and MapReduce programming.

In this article, we will first take a look at the components that make up Apache Hadoop and then some of the integrated systems and 
databases.

1. Components of Apache Hadoop
Hadoop, as a whole, consists of the following parts:
Hadoop Distributed File System – Abbreviated as HDFS, it is primarily a file system similar to many of the already existing ones. 
However, it is also a virtual file system.

Hadoop MapReduce – MapReduce is mainly the programming aspect of Hadoop that allows processing of large volumes of data.

HBASE – HBASE happens to be a layer that sits atop the HDFS and has been developed by means of the Java programming language. HBASE
primarily
has the following aspects –
•	Non relational
•	Highly scalable
•	Fault tolerance
Every single row that exists in HBASE is identified by means of a key. The number of columns is also not defined, but rather grouped into 
column families.

Zookeeper – This is basically a centralized system that maintains –
•	Configuration information
•	Naming information
•	Synchronization information

Solr/Lucene – This is nothing but a search engine. Its libraries are developed by Apache and required over 10 years to be developed in
its present robust form.

Programming Languages – There are basically two programming languages that are identified as original Hadoop programming languages,
•	Hive
•	PIG

2. Systems for integrated Hadoop operations
Most enterprise vendors have their very own Hadoop products that comprise of database as well as analytical offerings. These offerings 
also do not require you to source Hadoop from elsewhere, but rather provide it as a core aspect of their solutions.
Some of these are –
EMC Greenplum
Greenplum happens to be a pretty new entrant in the enterprise business and has a reputation for being a strong provider of analytics. 
It comes as a Unified Analytics Platform, which consists of –
•	Greenplum database – meant for use on structured data
•	Greenplum HD – Its Hadoop distribution
•	Chorus – A productivity layer for Data Science teams.

IBM
IBM’s enterprise distribution for Hadoop is known as Infosphere BigInsights. It implements an array of features for Hadoop, such as –
•	Tools for management
•	Tools for administration
•	It also comprises of a textual data analysis tools that help in the resolution of entities, such as identifying people, phone numbers, addresses and more.

Microsoft
Hadoop forms the core part of Microsoft’s big data offering. Pursuing an integrated approach, it plans to make available big data over 
its tool suite for analytics.

Oracle
Oracle entered into the world of big data with an appliance based approach in the form of Big Data Appliance. This ensures easy Hadoop integration, and 
comes along with the new NoSQL database, which allows for analytics and also has connections to Oracle databases and the Exadata warehousing lineup. NoSQL 
is also known as a scalable key value-based database offering.






